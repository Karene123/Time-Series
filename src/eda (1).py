# -*- coding: utf-8 -*-
"""eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p9JF6wGVTz9-PbmfvB2WBNYkQtaSqRU8

# DATA CLEANING AND DATA PROCESSING
"""

#!git clone https://github.com/Karene123/Time-Series.git

"""# Data Collection"""

!pip install sktime

# === System Utilities ===
import os

# === Core Libraries ===
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# === Forecasting Utilities (sktime) ===
import sktime
from sktime.forecasting.base import ForecastingHorizon
from sktime.forecasting.compose import make_reduction, TransformedTargetForecaster
from sktime.forecasting.naive import NaiveForecaster
from sktime.forecasting.trend import PolynomialTrendForecaster
from sktime.transformations.series.detrend import Detrender, Deseasonalizer
from sktime.forecasting.all import (
    Deseasonalizer, Detrender, temporal_train_test_split,
    mean_absolute_percentage_error as mape,
    mean_squared_percentage_error as mspe,
    mean_squared_error as mse,
    ForecastingHorizon,
    NaiveForecaster,
    TransformedTargetForecaster,
    PolynomialTrendForecaster
)
from sktime.performance_metrics.forecasting import (
    MeanAbsolutePercentageError, MeanSquaredError, MeanAbsoluteScaledError
)

# === Statistical Modeling (statsmodels) ===
from statsmodels.tsa.filters.hp_filter import hpfilter
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller, kpss, grangercausalitytests
from statsmodels.tsa.api import VAR
from statsmodels.tsa.ar_model import AutoReg
from statsmodels.stats.api import (
    het_breuschpagan, het_goldfeldquandt, het_white,
    jarque_bera, omni_normtest, normal_ad
)
from statsmodels.stats.diagnostic import (
    kstest_normal, acorr_ljungbox
)
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.formula.api import ols
from statsmodels.tools.tools import add_constant
from statsmodels.graphics.gofplots import qqplot

# === Statistical Tests (scipy) ===
from scipy.stats import shapiro, kstest, normaltest, boxcox

filenames = ['CPIAUCSL(1).xlsx', 'FEDFUNDS(1).xlsx', 'GDPC1(1).xlsx', 'M2SL(1).xlsx', 'M2V(1).xlsx', 'DCOILBRENTEU(1).xlsx']

# create a for loop to collect the data
data_path = 'Time-Series/data/raw/'
dic = {}
for file in filenames:
  file_path = os.path.join(data_path, file)
  df = pd.read_excel(file_path)
  dic[file] = df

# Response variable
CPI_df = dic['CPIAUCSL(1).xlsx']
# Predictor variable
FEDFUNDS_df = dic['FEDFUNDS(1).xlsx']
GDP_df = dic['GDPC1(1).xlsx']
M2SL_df = dic['M2SL(1).xlsx']
M2V_df = dic['M2V(1).xlsx']

# Response Variable
#CPI_df = pd.read_excel('Time-Series/data/raw/CPIAUCSL(1).xlsx') #CPI for all urban customers

# Predictor variables
#FEDFUNDS_df = pd.read_excel('Time-Series/data/raw/FEDFUNDS(1).xlsx')
#GDP_df = pd.read_excel('Time-Series/data/raw/GDPC1(1).xlsx')
#M2SL_df = pd.read_excel('Time-Series/data/raw/M2SL(1).xlsx')
#M2V_df = pd.read_excel('Time-Series/data/raw/M2V(1).xlsx')
#DCOILBRENT_df = pd.read_excel('Time-Series/data/raw/DCOILBRENTEU(1).xlsx')

# Print all the dataframes
files = [CPI_df, FEDFUNDS_df, GDP_df, M2SL_df, M2V_df]
filenames = ['CPI_df', 'FEDFUNDS_df', 'GDP_df', 'M2SL_df', 'M2V_df']

for i in range(5):
  print(filenames[i])
  print(files[i])

"""# Missing Data"""

for filename, file in zip(filenames, files):
    print(f"\n=== {filename} ===")
    print("Missing values Table:")
    print(file.isnull().sum())

"""# Merge"""

# Merge all the columns
data = pd.merge(pd.merge(pd.merge(pd.merge(CPI_df, FEDFUNDS_df, on='observation_date', how='outer'), M2SL_df, on='observation_date', how='outer'), M2V_df, on='observation_date', how='outer') , GDP_df, on='observation_date', how='outer')
data

# Data Types
data.dtypes

# Convert the month column to an index
data.index = data['observation_date']
del data['observation_date']

print(data.head())

data.index

data.index.freq = 'MS'
data.index

# Plot the data to observe the years where it starts to have missing data
plt.figure(figsize=(10, 30))

plt.subplot(5, 1, 1)
sns.lineplot(data['CPIAUCSL'])
plt.title("CPI")

plt.subplot(5, 1, 2)
sns.lineplot(data['FEDFUNDS'])
plt.title("FEDFUNDS")

plt.subplot(5, 1, 3)
sns.lineplot(data['M2SL'])
plt.title("M2SL")

plt.subplot(5, 1, 4)
sns.lineplot(data['M2V'])
plt.title("M2V")

plt.subplot(5, 1, 5)
sns.lineplot(data['GDPC1'])
plt.title("GDPC1")

# Slicing the data to include only the years of interest
data_sliced = data.loc["19600101":"20241001", :]
print(data_sliced)

# Let's check again the number of missing values per column
data_sliced.isnull().sum()

# describe() shows a quick statistic summary of your data
data_sliced.describe()

"""# Linear Interpolation

"""

# Resample the columns
cols = ['M2V', 'GDPC1']
# Resample each column separately (interpolation)
for col in cols:
    data_sliced[col] = data_sliced[col].interpolate()

print(data_sliced)

data_sliced.describe()

# Distribution of CPI varible (right skewed)
fig1 = px.histogram(data_sliced, x='CPIAUCSL')
fig1.show()

# Distribution of FEDFUNDS variable (right skewed)
fig2 = px.histogram(data_sliced, x='FEDFUNDS')
fig2.show()

# Distribution of M2SL variable (right skewed)
fig3 = px.histogram(data_sliced, x='M2SL')
fig3.show()

# Distribution of M2V	 variable (right skewed)
fig4 = px.histogram(data_sliced, x='M2V')
fig4.show()

# Distribution of GDPC1 variable (right skewed)
fig5 = px.histogram(data_sliced, x='GDPC1')
fig5.show()

"""# Data Diagnosis"""

CPIAUCSL_cyclic, CPIAUCSL_trend = hpfilter(data_sliced['CPIAUCSL'])

fig, ax = plt.subplots(1,2, figsize=(16, 4))
CPIAUCSL_cyclic.plot(ax=ax[0], title='CPIAUCSL Cyclic Component')
CPIAUCSL_trend.plot(ax=ax[1], title='CPIAUCSL Trend Component')
plt.show()

FEDFUNDS_cyclic, FEDFUNDS_trend = hpfilter(data_sliced['FEDFUNDS'])

fig, ax = plt.subplots(1,2, figsize=(16, 4))
FEDFUNDS_cyclic.plot(ax=ax[0], title='FEDFUNDS Cyclic Component')
FEDFUNDS_trend.plot(ax=ax[1], title='FEDFUNDS Trend Component')
plt.show()

M2SL_cyclic, M2SL_trend = hpfilter(data_sliced['M2SL'])

fig, ax = plt.subplots(1,2, figsize=(16, 4))
M2SL_cyclic.plot(ax=ax[0], title='M2SL Cyclic Component')
M2SL_trend.plot(ax=ax[1], title='M2SL Trend Component')
plt.show()

M2V_cyclic, M2V_trend = hpfilter(data_sliced['M2V'])

fig, ax = plt.subplots(1,2, figsize=(16, 4))
M2V_cyclic.plot(ax=ax[0], title='M2V Cyclic Component')
M2V_trend.plot(ax=ax[1], title='M2V Trend Component')
plt.show()

GDP_cyclic, GDP_trend = hpfilter(data_sliced['GDPC1'])

fig, ax = plt.subplots(1,2, figsize=(16, 4))
GDP_cyclic.plot(ax=ax[0], title='GDPC1 Cyclic Component')
GDP_trend.plot(ax=ax[1], title='GDPC1 Trend Component')
plt.show()

# Splitting the data between training and testing
data_train = data_sliced.loc[:'2020']
data_test = data_sliced.loc['2021':]
data_train.index

# save the dataset
#data_test.to_csv('Time-Series/data/processed/final_data_processed_test.csv', index=True)

#data_train.to_csv('Time-Series/data/processed/final_data_processed_train.csv', index=True)