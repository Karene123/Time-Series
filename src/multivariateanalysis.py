# -*- coding: utf-8 -*-
"""Multivariateanalysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g0UX5YvY0XJxQOAqTVaosR9Ww29WYFzP
"""



"""# Multivariate Analysis comparison"""

# Granger between CPI and FEDFUNDS
granger = grangercausalitytests(datatrain_sc[['CPIAUCSL', 'FEDFUNDS']], maxlag=20)

# Granger between CPI and M2SL
granger = grangercausalitytests(datatrain_sc[['CPIAUCSL','M2SL']], maxlag=20)

# Granger between CPI and M2V
granger = grangercausalitytests(datatrain_sc[['CPIAUCSL', 'M2V']], maxlag=20)

# Granger between CPI and GDPC1
granger = grangercausalitytests(datatrain_sc[['CPIAUCSL', 'GDPC1']], maxlag=20)



"""## SARIMA"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.tools import diff
import statsmodels.api as sm
import pandas as pd
import numpy as np

# import pmdarima as pm  # Automatically discover the optimal order for an ARIMA model

import warnings
warnings.filterwarnings('ignore')

import time
start2 = time.time()

# Assume `data` is a pandas Series or DataFrame
split_index = int(len(datatrain_sc) * 0.9)

train = datatrain_sc.iloc[:split_index]
test = datatrain_sc.iloc[split_index:]

print(f'\nTrain: {train.shape}')
print(f'\nTest: {test.shape}')

y_sliced = train[['CPIAUCSL']]

y_sliced

x_sliced = train[['FEDFUNDS', 'M2SL', 'M2V', 'GDPC1']]

x_sliced_removed_m2l = train[['FEDFUNDS', 'M2V', 'GDPC1']]

x_sliced_removed_gdp = train[['FEDFUNDS', 'M2V', 'M2SL']]

x_sliced_removed_fedf_gdp = train[['M2V', 'M2SL']]

x_sliced_removed_fedf_m2sl = train[['M2V', 'GDPC1']]

x_sliced_removed_fedf_m2sl_gdp = train[['M2V']]

x_sliced_removed_fedf_m2sl_m2v = train[['GDPC1']]

x_sliced_removed_fedf_gdp_m2v = train[['M2SL']]

# running the model including the two correlated variables
from statsmodels.tsa.statespace.sarimax import SARIMAX

model = SARIMAX(y_sliced,
                exog = x_sliced,
                order=(1,1,1),
                seasonal_order=(1,1,1,12),
                enforce_stationarity=False,
                enforce_invertibility=False)

results = model.fit()
print(results.summary())

# running the model including only one of the correlated variables

model = SARIMAX(y_sliced,
                exog = x_sliced_removed_m2l,
                order=(1,1,1),
                seasonal_order=(1,1,1,12),
                enforce_stationarity=False,
                enforce_invertibility=False)

results = model.fit()
print(results.summary())

# running the model including only one of the correlated variables

model = SARIMAX(y_sliced,
                exog = x_sliced_removed_gdp,
                order=(1,1,1),
                seasonal_order=(1,1,1,12),
                enforce_stationarity=False,
                enforce_invertibility=False)

results = model.fit()
print(results.summary())

# running the model including only one of the correlated variables

model = SARIMAX(y_sliced,
                exog = x_sliced_removed_fedf_gdp,
                order=(1,1,1),
                seasonal_order=(1,1,1,12),
                enforce_stationarity=False,
                enforce_invertibility=False)

results = model.fit()
print(results.summary())

# running the model including only one of the correlated variables

model = SARIMAX(y_sliced,
                exog = x_sliced_removed_fedf_m2sl,
                order=(1,1,1),
                seasonal_order=(1,1,1,12),
                enforce_stationarity=False,
                enforce_invertibility=False)

results = model.fit()
print(results.summary())

# running the model including only one of the correlated variables

model = SARIMAX(y_sliced,
                exog = x_sliced_removed_fedf_m2sl_m2v,
                order=(1,1,1),
                seasonal_order=(1,1,1,12),
                enforce_stationarity=False,
                enforce_invertibility=False)

results = model.fit()
print(results.summary())

# running the model including only one of the correlated variables

model = SARIMAX(y_sliced,
                exog = x_sliced_removed_fedf_m2sl_gdp,
                order=(1,1,1),
                seasonal_order=(1,1,1,12),
                enforce_stationarity=False,
                enforce_invertibility=False)

results = model.fit()
print(results.summary())

# running the model including only one of the correlated variables

model = SARIMAX(y_sliced,
                exog = x_sliced_removed_fedf_gdp_m2v,
                order=(1,1,1),
                seasonal_order=(1,1,1,12),
                enforce_stationarity=False,
                enforce_invertibility=False)

results = model.fit()
print(results.summary())

"""## Machine Learning"""

def create_lagged_features(df, target_col, lags=12):
    df = df.copy()
    for col in df.columns:
        for lag in range(1, lags+1):
            df[f'{col}_lag{lag}'] = df[col].shift(lag)
    return df.dropna()

datatrain_sc_lagged = create_lagged_features(df = datatrain_sc, target_col='CPIAUCSL', lags=12)

# Split training data
y_train =  datatrain_sc['CPIAUCSL']
X_train =  datatrain_sc.drop(columns='CPIAUCSL')

# Split test data
y_test =  datatest_sc['CPIAUCSL']
X_test =  datatest_sc.drop(columns='CPIAUCSL')

# Check the index
datatrain_sc.index

# Split into train/test using 48 months = 12 months x 4
y_train = y_train.iloc[:-48]
y_test = y_test.iloc[-48:]

X_train = X_train.iloc[:-48]
X_test = X_test.iloc[-48:]

length_y_train = len(y_train)

length_y_test = len(y_test)

# Convert to datetimeindex to avoid issues with the remaining part
y_train.index = pd.date_range(start=y_train.index[0], periods=length_y_train, freq='M')
y_test.index = pd.date_range(start=y_test.index[0], periods= length_y_test, freq='M')
X_train.index = y_train.index
X_test.index = y_test.index

# Gradient Boosting Regressor
model = make_reduction(
    GradientBoostingRegressor(),
    strategy="recursive",
    window_length=12,
    scitype="tabular-regressor")

model.fit(y_train, X=X_train)
fh = ForecastingHorizon(y_test.index, is_relative=False)
y_pred_gb = model.predict(fh=fh, X=X_test)
if isinstance(y_test.index, pd.PeriodIndex):
    y_test.index = y_test.index.to_timestamp()
if isinstance(y_pred_gb.index, pd.PeriodIndex):
    y_pred.index = y_pred_gb.index.to_timestamp()

# Plot
plt.figure(figsize=(12, 5))
plt.plot(y_test, label="Actual", marker='o')
plt.plot(y_pred_gb, label="Forecast", marker='x')
plt.title("Gradient Boosting Regressor Forecast vs Actual")
plt.xlabel("Month")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Gradient Boosting Regressor
model = make_reduction(
    GradientBoostingRegressor(),
    strategy="recursive",
    window_length=12,
    scitype="tabular-regressor")

model.fit(y_train, X = X_train)

fh = ForecastingHorizon(y_test.index, is_relative=False)

y_pred_gb = model.predict(fh=fh, X=X_test)
if isinstance(y_test.index, pd.PeriodIndex):
    y_train.index = y_test.index.to_timestamp()
if isinstance(y_pred_gb.index, pd.PeriodIndex):
    y_pred.index = y_pred_gb.index.to_timestamp()

# Plot
plt.figure(figsize=(12, 5))
plt.plot(y_test, label="Actual", marker='o')
plt.plot(y_pred_gb, label="Forecast", marker='x')
plt.title("Gradient Boosting Regressor Forecast vs Actual")
plt.xlabel("Month")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Linear Regression
model = make_reduction(
    LinearRegression(),
    strategy="recursive",
    window_length=12,
    scitype="tabular-regressor"
)

model.fit(y_train, X=X_train)
fh = ForecastingHorizon(y_test.index, is_relative=False)
y_pred_lr = model.predict(fh=fh, X=X_test)

if isinstance(y_test.index, pd.PeriodIndex):
    y_test.index = y_test.index.to_timestamp()
if isinstance(y_pred_lr.index, pd.PeriodIndex):
    y_pred.index = y_pred_lr.index.to_timestamp()

# Plot
plt.figure(figsize=(12, 5))
plt.plot(y_test, label="Actual", marker='o')
plt.plot(y_pred_lr, label="Forecast", marker='x')
plt.title("Linear Regression Forecast vs Actual")
plt.xlabel("Month")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# KNN
model = make_reduction(
    KNeighborsRegressor(n_neighbors=5),
    strategy="recursive",
    window_length=12,
    scitype="tabular-regressor"
)

model.fit(y_train, X=X_train)

fh = ForecastingHorizon(y_test.index, is_relative=False)

y_pred_knn = model.predict(fh=fh, X=X_test)

# Plot
plt.figure(figsize=(12, 5))
plt.plot(y_test, label="Actual", marker='o')
plt.plot(y_pred_knn, label="Forecast", marker='x')
plt.title("K-Nearest Neighbors Forecast vs Actual")
plt.xlabel("Month")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# RandomForest
model = make_reduction(
    RandomForestRegressor(),
    strategy="recursive",
    window_length=12,
    scitype="tabular-regressor"
)

model.fit(y_train, X=X_train)
fh = ForecastingHorizon(y_test.index, is_relative=False)
y_pred_rf = model.predict(fh=fh, X=X_test)

# Plot
plt.figure(figsize=(12, 5))
plt.plot(y_test, label="Actual", marker='o')
plt.plot(y_pred_rf, label="Forecast", marker='x')
plt.title("RandomForestRegressor Forecast vs Actual")
plt.xlabel("Month")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

mse = MeanSquaredError()
mape = MeanAbsolutePercentageError()
mase = MeanAbsoluteScaledError()

def evaluate(df, y_train, sort_by='MASE'):
    evals = pd.DataFrame(index=['sMAPE', 'MAPE', 'RMSE', 'MASE'])
    y_truth = pd.Series(df['y'].values, index=df['y'].index, name='y')
    y_predicted = df.drop(columns='y')
    for p in y_predicted.columns:
        y_pred = pd.Series(y_predicted[p].values, index=y_truth.index, name='y')
        evals.loc['sMAPE', p] = mape(y_truth, y_pred, symmetric=True)
        evals.loc['MAPE', p] = mape(y_truth, y_pred, symmetric=False)
        evals.loc['RMSE', p] = np.sqrt(mse(y_truth, y_pred))
        evals.loc['MASE', p] = mase(y_truth, y_pred, y_train=y_train)
    return evals.T.sort_values(by=sort_by)

comparison_df = pd.DataFrame({
    "y": y_test,  # true values
    "Linear Regression": y_pred_lr,
    "Random Forest": y_pred_rf,
    "Gradient Boosting": y_pred_gb,
    "K-Nearest Neighbors": y_pred_knn
})

evaluate(comparison_df, y_train, sort_by='RMSE')

"""## Neural Network"""

#import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import MinMaxScaler

# Scale X and y
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))
X_test_scaled = scaler_X.transform(X_test)

# Define MLP model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1)
])

# Compile and train
model.compile(optimizer='adam', loss='mse')
model.fit(X_train_scaled, y_train_scaled, epochs=100, verbose=0)

# Predict
y_pred_scaled = model.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled)

# Convert to pandas Series for plotting
y_pred_nn = pd.Series(y_pred.flatten(), index=y_test.index, name='Neural Net')

# Optional: add to comparison_df
comparison_df['Neural Net'] = y_pred_nn

"""## AUTO ARIMA"""

n = len(y_test)
fh = ForecastingHorizon(np.arange(n) + 1)
arima = auto_arima(y=y_train, X=X_train, seasonal=True, m=12, suppress_warnings=True)

arima.summary()

y_pred_arima = arima.predict(n, X = X_test)
y_pred_arima = pd.Series(y_pred_arima, index=y_test.index, name='ARIMA')
y_pred_arima.head()

import statsmodels.api as sm
sm.tsa.SARIMAX(
    endog=y_train,
    exog=X_train,
    order=(1, 0, 1),
    seasonal_order=(1, 0, 1, 12)
).fit(disp=False).predict(start=len(y_train), end=len(y_train)+len(X_test)-1, exog=X_test)

#y_pred_arima = pd.Series(y_pred_arima, index=y_test.index, name='ARIMA')

comparison_df['ARIMA'] = y_pred_arima
evaluate(comparison_df, y_train, sort_by='MASE')

y_pred_arima

ax = y_train.loc['1960':].plot(style='--', alpha=0.35)
y_test['ARIMA'].plot(ax=ax, style='k-.')
y_test['K-NN Regressor'].plot(ax=ax, style='k-o')

# test['Naive Forecaster'].plot(ax=ax, style='.-')
plt.legend(['train', 'ARIMA', 'KNN Regressor'])
plt.show()

# Create the base plot using the training data
ax = train.loc['2011':].plot(style='--', alpha=0.35, figsize=(12, 5))

# Plot ARIMA forecast
test['ARIMA'].plot(ax=ax, style='k-', label='ARIMA')

# Plot KNN forecast
test['K-NN Regressor'].plot(ax=ax, style='k-o', label='K-NN Regressor')

# OPTIONAL: Add more models
# test['Naive Forecaster'].plot(ax=ax, style='--', label='Naive Forecaster')

# Add legend and styling
plt.legend()
plt.title("Forecast Comparison: ARIMA vs K-NN Regressor")
plt.xlabel("Month")
plt.ylabel("Value")
plt.grid(True)
plt.tight_layout()
plt.show()

"""## Comment
- Residuals are not normally distributed with high Jarque-Bera and its low p-values
- P-values for Ljung Box is greater than 0.05 which indicates that autocorrelation can be challenged
- Constant variance is not present
- With Kurtosis, it indicates outliers and sharp break

## Comment
Aside from FEDFUNDS, most of the predictor variables affect CPI across lags.

## Comment
- It appears that none of the lags for FEDFUNDS explain as expected from running grangercausalitytest
- a change in (M2SL) takes 3 months to impact CPI but it does not impact short term nor long term
- CPI is automatically impacted by M2V (from lag 1 to lag 4) but this impact fades after month 5.
- A change in GDP, takes 7 months (lag 7) to impact CPI.
- FOR GDPC1 and M2V, both have cyclical component every 4 months due to the linear interpolation.

Looking at the correlation matrix, it appears that M2V and GDPC1 are highly correlated with 0.84. The rest of the variables are uncorrelated.
"""

